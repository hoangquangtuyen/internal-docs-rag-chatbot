# ğŸ“š Internal Docs RAG Chatbot

A serverless **Retrieval-Augmented Generation (RAG)** chatbot deployed on **AWS Lambda using Docker container images**, designed to answer questions from internal documents efficiently and cost-effectively.

Built with **FastAPI + FAISS**, focusing on clean architecture, serverless deployment, and real-world backend practices.

---

## ğŸš€ Key Features

- Serverless deployment using **AWS Lambda (Container Image)**
- Dockerized **FastAPI** application (Lambda-compatible)
- Full RAG pipeline: document ingestion â†’ chunking â†’ embeddings â†’ vector search
- **Mock LLM mode** for local testing and cost-free development
- Simple, maintainable backend structure (no unnecessary complexity)

---

## ğŸ§  Architecture Overview

Client
â†“
FastAPI (AWS Lambda)
â†“
FAISS Vector Store
â†“
LLM (Mock / Local)

- Documents are ingested and indexed into **FAISS**
- User queries retrieve relevant chunks
- Context is passed to an **LLM layer** to generate answers

---

## ğŸ“‚ Project Structure

```text
internal-docs-rag-chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # FastAPI entry point (Mangum Lambda handler)
â”‚   â”œâ”€â”€ ingest.py           # Document ingestion & indexing
â”‚   â”œâ”€â”€ chat_local.py       # Local LLM logic
â”‚   â”œâ”€â”€ chat_mock.py        # Mock LLM (no API cost)
â”‚   â”œâ”€â”€ config.py           # Environment configuration
â”‚   â””â”€â”€ aws/                # AWS-specific helpers
â”œâ”€â”€ data/                   # Internal documents
â”œâ”€â”€ Dockerfile              # Lambda-compatible Docker image
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-lambda.txt
â”œâ”€â”€ test_fastapi.py         # API testing
â””â”€â”€ README.md
```

## âš™ï¸ Tech Stack

- Language: Python 3.10+

- API Framework: FastAPI

- Vector Search: FAISS

- Deployment: AWS Lambda, Amazon ECR

- Containerization: Docker

## ğŸ³ Run Locally (Lambda Runtime)

### Build image

```bash
docker build -t rag-chatbot-lambda .
Run container
docker run --rm -p 9000:8080 rag-chatbot-lambda
```
### Test Lambda locally
```bash
curl -X POST http://localhost:9000/2015-03-31/functions/function/invocations \
  -H "Content-Type: application/json" \
  -d '{"httpMethod":"GET","path":"/health"}'
``` 

## â˜ï¸ AWS Deployment
Build Docker image locally

Push image to Amazon ECR

Create AWS Lambda function (Image type)

Test via Lambda Console or Function URL

âœ… Successfully deployed and tested on AWS Lambda

Deployment is done manually via Docker + AWS CLI

### ğŸ§ª Development Mode
`chat_mock.py` allows running the full RAG flow without calling external LLM APIs

Suitable for:

Local testing

Demonstration

Cost-free development

## ğŸ¯ What This Project Demonstrates
Practical RAG system implementation

Real-world Docker â†’ ECR â†’ Lambda workflow

Understanding of serverless constraints

Ability to design systems with cost-awareness and simplicity

## ğŸ‘¤ Author
HoÃ ng Tuyáº¿n
Project built for hands-on learning and job applications.