ğŸ“š Internal Docs RAG Chatbot

Má»™t project RAG (Retrieval-Augmented Generation) Chatbot dÃ¹ng dá»¯ liá»‡u ná»™i bá»™ (PDF, DOCX, Markdown, TXT, â€¦), cÃ³ thá»ƒ cháº¡y local, Docker, vÃ  deploy trÃªn AWS Lambda. Project Ä‘Æ°á»£c thiáº¿t káº¿ theo hÆ°á»›ng modular, dá»… má»Ÿ rá»™ng, vÃ  phÃ¹ há»£p Ä‘á»ƒ Ä‘Æ°a vÃ o CV / Portfolio.

ğŸš€ TÃ­nh nÄƒng chÃ­nh

ğŸ” RAG pipeline: ingest tÃ i liá»‡u â†’ embedding â†’ lÆ°u vectorstore â†’ truy váº¥n + sinh cÃ¢u tráº£ lá»i

ğŸ¤– 2 cháº¿ Ä‘á»™ chat:

chat_local.py: dÃ¹ng LLM tháº­t (Gemini / HuggingFace / â€¦)

chat_mock.py: mock LLM (khÃ´ng cáº§n API key, phÃ¹ há»£p demo & deploy Lambda)

ğŸ“¦ Vectorstore FAISS (offline, nháº¹, nhanh)

ğŸ³ Docker-ready (cháº¡y local & Lambda container)

â˜ï¸ AWS Lambda compatible (image-based deployment)

ğŸ” Quáº£n lÃ½ cáº¥u hÃ¬nh & API key qua .env

ğŸ—ï¸ Kiáº¿n trÃºc tá»•ng quan
User Query
   â†“
Retriever (FAISS)
   â†“
Relevant Chunks
   â†“
LLM (Local / Mock / API)
   â†“
Final Answer

Mock mode giÃºp tÃ¡ch biá»‡t business logic vÃ  LLM provider, ráº¥t phÃ¹ há»£p trong mÃ´i trÆ°á»ng khÃ´ng cÃ³ chi phÃ­ API.

ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c
internal-docs-rag-chatbot/
â”‚
â”œâ”€â”€ data/                   # Dá»¯ liá»‡u Ä‘áº§u vÃ o (pdf, docx, md, txtâ€¦)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py               # Entry point (FastAPI / Lambda handler)
â”‚   â”œâ”€â”€ ingest.py            # Ingest & build vectorstore
â”‚   â”œâ”€â”€ chat_local.py        # Chat vá»›i LLM tháº­t
â”‚   â”œâ”€â”€ chat_mock.py         # Chat mock (khÃ´ng cáº§n API key)
â”‚   â”œâ”€â”€ config.py            # Load config & env
â”‚   â””â”€â”€ aws/
â”‚       â”œâ”€â”€ dist/            # Build artifacts cho Lambda
â”‚       â””â”€â”€ install/         # Dependencies Lambda
â”‚
â”œâ”€â”€ requirements.txt         # Dependencies local
â”œâ”€â”€ requirements-lambda.txt  # Dependencies cho Lambda
â”œâ”€â”€ Dockerfile               # Docker & Lambda image
â”œâ”€â”€ test_fastapi.py          # Test API
â”œâ”€â”€ response.json            # Sample response
â”œâ”€â”€ .env.example             # Máº«u biáº¿n mÃ´i trÆ°á»ng
â””â”€â”€ README.md
âš™ï¸ CÃ i Ä‘áº·t & cháº¡y local
1ï¸âƒ£ Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
2ï¸âƒ£ CÃ i dependencies
pip install -r requirements.txt
3ï¸âƒ£ Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng

Táº¡o file .env:

LLM_PROVIDER=gemini
GOOGLE_API_KEY=your_api_key_here

âš ï¸ CÃ³ thá»ƒ khÃ´ng cáº§n API key náº¿u dÃ¹ng chat_mock.py

ğŸ“¥ Ingest dá»¯ liá»‡u
python src/ingest.py

Script sáº½:

Load tÃ i liá»‡u trong data/

Split text

Táº¡o embedding

LÆ°u FAISS vectorstore

ğŸ’¬ Cháº¡y chatbot
Mock mode (khuyáº¿n nghá»‹ Ä‘á»ƒ demo / Lambda)
python src/chat_mock.py
Local LLM / API mode
python src/chat_local.py
ğŸ³ Cháº¡y báº±ng Docker
docker build -t rag-chatbot .
docker run -p 8000:8000 rag-chatbot

Test:

http://localhost:8000/docs
â˜ï¸ Deploy AWS Lambda (Container Image)

Base image: public.ecr.aws/lambda/python

Entry point: src/app.py

KhÃ´ng phá»¥ thuá»™c API key khi dÃ¹ng mock mode

ğŸ‘‰ PhÃ¹ há»£p cho free-tier / demo / interview project

ğŸ‘¤ TÃ¡c giáº£
HoÃ ng Tuyáº¿n