# ğŸ“š Internal Docs RAG Chatbot

A serverless **Retrieval-Augmented Generation (RAG)** chatbot deployed on **AWS Lambda using Docker container images**, designed to answer questions from internal documents efficiently and cost-effectively.

Built with **FastAPI + FAISS + LangChain**, focusing on clean architecture, serverless deployment, and real-world backend practices.

---

## ğŸš€ Key Features

- Serverless deployment using **AWS Lambda (Container Image)**
- Public **REST API** exposed via **Amazon API Gateway**
- Dockerized **FastAPI** application (Lambda-compatible)
- **Full RAG pipeline implemented using LangChain**: document ingestion, text chunking, embedding generation, and FAISS-based vector search
- **Mock LLM mode** for local testing and cost-free development
- Simple, maintainable backend structure (no unnecessary complexity)

---

## ğŸ§  Architecture Overview

Client
â†“
Amazon API Gateway
â†“
FastAPI (AWS Lambda via Mangum)
â†“
FAISS Vector Store
â†“
LLM (Mock / Local)

Flow:

Internal documents are ingested and indexed into FAISS

User requests hit API Gateway and are forwarded to Lambda

FastAPI handles routing and retrieval logic

Relevant document chunks are retrieved

Context is passed to an LLM layer to generate answers

---

## ğŸ“‚ Project Structure

```text
internal-docs-rag-chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # FastAPI entry point (Mangum Lambda handler)
â”‚   â”œâ”€â”€ ingest.py           # Document ingestion & indexing
â”‚   â”œâ”€â”€ chat_local.py       # Local LLM logic
â”‚   â”œâ”€â”€ chat_mock.py        # Mock LLM (no API cost)
â”‚   â”œâ”€â”€ config.py           # Environment configuration
â”‚   â””â”€â”€ aws/                # AWS-specific helpers
â”œâ”€â”€ data/                   # Internal documents
â”œâ”€â”€ Dockerfile              # Lambda-compatible Docker image
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-lambda.txt
â”œâ”€â”€ test_fastapi.py         # API testing
â””â”€â”€ README.md
```

## âš™ï¸ Tech Stack

- Language: Python 3.10+

- API Framework: FastAPI

- RAG Framework: LangChain

- Vector Search: FAISS

- API Layer: Amazon API Gateway (REST API)

- Deployment: AWS Lambda, Amazon ECR

- Container Registry: Amazon ECR

- Containerization: Docker

## ğŸ³ Run Locally (Lambda Runtime)

### Build image

```bash
docker build -t rag-chatbot-lambda .
Run container
docker run --rm -p 9000:8080 rag-chatbot-lambda
```
### Test Lambda locally
```bash
curl -X POST http://localhost:9000/2015-03-31/functions/function/invocations \
  -H "Content-Type: application/json" \
  -d '{"httpMethod":"GET","path":"/health"}'
``` 

## â˜ï¸ AWS Deployment
1. Build Docker image locally

2. Push image to Amazon ECR

3. Create AWS Lambda function (Image type)

4. Configure Amazon API Gateway to route HTTP requests to Lambda

5. Deploy API Gateway stage (/prod)

6. Test endpoints via API Gateway

## Available Endpoints
```
GET  /health
POST /chat 
```
âœ… Successfully deployed and tested on AWS Lambda behind API Gateway
âœ… Deployment handled manually via Docker + AWS CLI

## ğŸ§ª Development Mode
`chat_mock.py` allows running the full RAG flow without calling external LLM APIs

Suitable for:

- Local testing

- Demonstration

- Cost-free development

## ğŸ¯ What This Project Demonstrates
- Practical RAG system implementation using LangChain

- Real-world Docker â†’ ECR â†’ Lambda â†’ API Gateway workflow

- Understanding of serverless constraints

- REST API design and request routing via Amazon API Gateway

- Ability to build cost-aware, production-style backend systems
## ğŸ‘¤ Author
HoÃ ng Tuyáº¿n
Project built for hands-on learning and job applications.