import sys
import os
import torch

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from config import (
    VECTORSTORE_DIR,
    EMBEDDING_MODEL_NAME,
    TOP_K,
    TEMPERATURE,
    MAX_OUTPUT_TOKENS,
    SEARCH_TYPE,
    MMR_DIVERSITY
)

# =========================
# üéØ PROMPT TEMPLATE
# =========================
PROMPT_TEMPLATE = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu n·ªôi b·ªô.

NGUY√äN T·∫ÆC:
1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ t√†i li·ªáu b√™n d∆∞·ªõi
2. N·∫øu kh√¥ng c√≥ th√¥ng tin, n√≥i r√µ: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu"
3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, s√∫c t√≠ch

T√ÄI LI·ªÜU THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {input}

TR·∫¢ L·ªúI:
"""

# =========================
# üöÄ INIT CHATBOT
# =========================
def initialize_chatbot():
    try:
        print("üîÑ Loading embedding model...")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )

        print("üîÑ Loading vectorstore...")
        if not VECTORSTORE_DIR.exists():
            raise FileNotFoundError(
                f"Vectorstore not found at {VECTORSTORE_DIR}. "
                "Please run ingest.py first."
            )

        vectorstore = FAISS.load_local(
            str(VECTORSTORE_DIR),
            embeddings,
            allow_dangerous_deserialization=True
        )

        # =========================
        # üîç RETRIEVER
        # =========================
        if SEARCH_TYPE == "mmr":
            retriever = vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": TOP_K,
                    "fetch_k": TOP_K * 2,
                    "lambda_mult": MMR_DIVERSITY
                }
            )
            print(f"‚úÖ Using MMR retriever (diversity={MMR_DIVERSITY})")
        else:
            retriever = vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": TOP_K}
            )
            print(f"‚úÖ Using similarity retriever (top_k={TOP_K})")

        # =========================
        # ü§ñ LOCAL LLM (QWEN)
        # =========================
        print("üîÑ Loading local LLM: Qwen2.5-3B-Instruct")

        model_id = "Qwen/Qwen2.5-3B-Instruct"

        tokenizer = AutoTokenizer.from_pretrained(model_id)

        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        text_gen_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=MAX_OUTPUT_TOKENS,
            temperature=TEMPERATURE,
            do_sample=True
        )

        llm = HuggingFacePipeline(pipeline=text_gen_pipeline)

        # =========================
        # üîó LCEL RAG CHAIN
        # =========================
        prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

        rag_chain = (
            {
                "context": retriever,
                "input": RunnablePassthrough()
            }
            | prompt
            | llm
        )

        print("‚úÖ Chatbot ready (Local LLM)")
        return rag_chain

    except Exception as e:
        print(f"‚ùå Error initializing chatbot: {e}")
        sys.exit(1)

# =========================
# üí¨ CHAT LOOP
# =========================
def chat():
    print("\n" + "=" * 60)
    print("ü§ñ Internal Docs RAG Chatbot (LOCAL LLM)")
    print("=" * 60)

    rag_chain = initialize_chatbot()

    print("\nüí° Tips:")
    print("  ‚Ä¢ Type 'exit' or 'quit' to quit")
    print("  ‚Ä¢ Type 'clear' to clear screen")
    print("=" * 60 + "\n")

    while True:
        try:
            query = input("‚ùì Your question: ").strip()

            if not query:
                continue

            if query.lower() in ["exit", "quit", "q"]:
                print("\nüëã Goodbye!")
                break

            if query.lower() == "clear":
                os.system("cls" if os.name == "nt" else "clear")
                continue

            print("\nüîç Searching documents...")
            response = rag_chain.invoke(query)

            print("\n" + "‚îÄ" * 60)
            print("ü§ñ Answer:")
            print("‚îÄ" * 60)
            print(response)
            print("=" * 60 + "\n")

        except KeyboardInterrupt:
            print("\nüëã Interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"\n‚ùå Error: {e}\n")

# =========================
# ‚ñ∂Ô∏è MAIN
# =========================
if __name__ == "__main__":
    chat()
